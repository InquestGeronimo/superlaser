<div align="center">
    <img width="400" height="350" src="/img/laser.webp">
</div>

<h1 align="center">
  <em>SuperLaser</em>
</h1>



**SuperLaser** offers a comprehensive suite of tools and scripts for deploying Large Language Models (LLMs) to RunPod's serverless infrastructure using Python. This approach facilitates efficient and scalable LLM inference, capitalizing on RunPod's robust serverless architecture. The setup is specifically designed to operate with the vLLM engine, ensuring a seamless integration and high-performance inference capabilities.

# Features <img align="center" width="30" height="29" src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExOTBqaWNrcGxnaTdzMGRzNTN0bGI2d3A4YWkxajhsb2F5MW84Z2dxaCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/26tOZ42Mg6pbTUPHW/giphy.gif">

- **Scalable Deployment**: Easily scale your LLM inference tasks with RunPod serverless capabilities.
- **Cost-Effective**: Optimize resource usage and reduce costs with serverless architecture.
- **Easy Integration**: Seamless integration with existing LLM workflows.

<!-- ### Prerequisites

Before you begin, ensure you have:

- A RunPod account.
- The Runpod CLI `runpodctl`
    - on Linux:

```bash
wget -qO- cli.runpod.net | sudo bash
```

# Install <img align="center" width="30" height="29" src="https://media.giphy.com/media/sULKEgDMX8LcI/giphy.gif">


```bash
pip install superlaser
```

# Inference <img align="center" width="30" height="29" src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMXV1bWFyMWxkY3JocjE1ZDMxMWZ5OHZtejFkbHpuZXdveTV3Z3BiciZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/bGgsc5mWoryfgKBx1u/giphy.gif"> -->


