LLM_Settings:
  MODEL_NAME: "mistralai/Mistral-7B-v0.1"
  MODEL_REVISION: None
  MAX_MODEL_LENGTH: 512
  BASE_PATH: "/runpod-volume"
  LOAD_FORMAT: "auto"
  HF_TOKEN: None
  QUANTIZATION: None
  TRUST_REMOTE_CODE: 0
  SEED: 0
  KV_CACHE_DTYPE: "auto"
  DTYPE: "auto"

Tokenizer_Settings:
  TOKENIZER_NAME: None
  TOKENIZER_REVISION: None
  CUSTOM_CHAT_TEMPLATE: None

System_GPU_and_Tensor_Parallelism_Settings:
  GPU_MEMORY_UTILIZATION: 0.95
  MAX_PARALLEL_LOADING_WORKERS: None
  BLOCK_SIZE: 16
  SWAP_SPACE: 4
  ENFORCE_EAGER: 0
  MAX_CONTEXT_LEN_TO_CAPTURE: 8192
  DISABLE_CUSTOM_ALL_REDUCE: 0

Streaming_Batch_Size_Settings:
  DEFAULT_BATCH_SIZE: 50
  DEFAULT_MIN_BATCH_SIZE: 1
  DEFAULT_BATCH_SIZE_GROWTH_FACTOR: 3

OpenAI_Settings:
  RAW_OPENAI_OUTPUT: 1
  OPENAI_SERVED_MODEL_NAME_OVERRIDE: None
  OPENAI_RESPONSE_ROLE: "assistant"

Serverless_Settings:
  MAX_CONCURRENCY: 300
  DISABLE_LOG_STATS: 1
  DISABLE_LOG_REQUESTS: 1